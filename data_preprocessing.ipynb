{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file_path = './part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv'\n",
    "train_raw_data = pd.read_csv(train_csv_file_path)\n",
    "\n",
    "test_csv_file_path = './part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv'\n",
    "test_raw_data = pd.read_csv(test_csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc: integer location [row, column]\n",
    "# store only data (not column name)\n",
    "# use iloc, not just slicing.\n",
    "train_data = train_raw_data.iloc[1:, :-1]\n",
    "train_label = train_raw_data.iloc[:, -1]\n",
    "\n",
    "test_data = test_raw_data.iloc[1:, :-1]\n",
    "test_label = test_raw_data.iloc[:, -1]\n",
    "\n",
    "# print(train_raw_data.columns) has label data\n",
    "\n",
    "# subsetting from one dataset is done with sklearn's train_test_split\n",
    "# features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if the two labels has the same content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unique_label = np.sort(train_label.unique())\n",
    "test_unique_label = np.sort(test_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(True) if len(train_unique_label) == len(test_unique_label) and np.array_equal(train_unique_label, test_unique_label) else print(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the distribution of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set distribution\n",
      "label\n",
      "DDoS-ICMP_Flood            36554\n",
      "DDoS-UDP_Flood             27626\n",
      "DDoS-TCP_Flood             23149\n",
      "DDoS-PSHACK_Flood          21210\n",
      "DDoS-SYN_Flood             20739\n",
      "DDoS-RSTFINFlood           20669\n",
      "DDoS-SynonymousIP_Flood    18189\n",
      "DoS-UDP_Flood              16957\n",
      "DoS-TCP_Flood              13630\n",
      "DoS-SYN_Flood              10275\n",
      "BenignTraffic               5600\n",
      "Mirai-greeth_flood          5016\n",
      "Mirai-udpplain              4661\n",
      "Mirai-greip_flood           3758\n",
      "DDoS-ICMP_Fragmentation     2377\n",
      "MITM-ArpSpoofing            1614\n",
      "DDoS-ACK_Fragmentation      1505\n",
      "DDoS-UDP_Fragmentation      1484\n",
      "DNS_Spoofing                 925\n",
      "Recon-HostDiscovery          697\n",
      "Recon-OSScan                 517\n",
      "Recon-PortScan               430\n",
      "DoS-HTTP_Flood               414\n",
      "VulnerabilityScan            210\n",
      "DDoS-HTTP_Flood              169\n",
      "DDoS-SlowLoris               106\n",
      "DictionaryBruteForce          63\n",
      "SqlInjection                  31\n",
      "BrowserHijacking              30\n",
      "CommandInjection              28\n",
      "Backdoor_Malware              22\n",
      "XSS                           18\n",
      "Uploading_Attack               8\n",
      "Recon-PingSweep                6\n",
      "Name: count, dtype: int64\n",
      "testing set distribution\n",
      "label\n",
      "DDoS-ICMP_Flood            33529\n",
      "DDoS-UDP_Flood             25343\n",
      "DDoS-TCP_Flood             20964\n",
      "DDoS-PSHACK_Flood          19373\n",
      "DDoS-SYN_Flood             19235\n",
      "DDoS-RSTFINFlood           19032\n",
      "DDoS-SynonymousIP_Flood    16798\n",
      "DoS-UDP_Flood              15500\n",
      "DoS-TCP_Flood              12326\n",
      "DoS-SYN_Flood               9314\n",
      "BenignTraffic               5200\n",
      "Mirai-greeth_flood          4728\n",
      "Mirai-udpplain              4308\n",
      "Mirai-greip_flood           3606\n",
      "DDoS-ICMP_Fragmentation     2132\n",
      "MITM-ArpSpoofing            1432\n",
      "DDoS-UDP_Fragmentation      1392\n",
      "DDoS-ACK_Fragmentation      1384\n",
      "DNS_Spoofing                 827\n",
      "Recon-HostDiscovery          652\n",
      "Recon-OSScan                 425\n",
      "Recon-PortScan               399\n",
      "DoS-HTTP_Flood               347\n",
      "VulnerabilityScan            143\n",
      "DDoS-HTTP_Flood              127\n",
      "DDoS-SlowLoris               100\n",
      "DictionaryBruteForce          57\n",
      "BrowserHijacking              30\n",
      "SqlInjection                  27\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              18\n",
      "XSS                           16\n",
      "Recon-PingSweep               10\n",
      "Uploading_Attack               5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"training set distribution\")\n",
    "train_label_counts = train_raw_data['label'].value_counts()\n",
    "print(train_label_counts)\n",
    "\n",
    "print(\"testing set distribution\")\n",
    "test_label_counts = test_raw_data['label'].value_counts()\n",
    "print(test_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_raw_data.drop('label', axis = 1)\n",
    "train_target = train_raw_data['label']\n",
    "\n",
    "test_features = test_raw_data.drop('label', axis = 1)\n",
    "test_target = test_raw_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For label, do label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target, train_target_mapping = pd.factorize(train_target)\n",
    "test_target, test_target_mapping = pd.factorize(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 2]\n",
      "Index(['DDoS-RSTFINFlood', 'DoS-TCP_Flood', 'DDoS-ICMP_Flood', 'DoS-UDP_Flood',\n",
      "       'DoS-SYN_Flood', 'Mirai-greeth_flood', 'DDoS-SynonymousIP_Flood',\n",
      "       'Mirai-udpplain', 'DDoS-SYN_Flood', 'DDoS-PSHACK_Flood',\n",
      "       'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'BenignTraffic', 'MITM-ArpSpoofing',\n",
      "       'DDoS-ACK_Fragmentation', 'Mirai-greip_flood', 'DoS-HTTP_Flood',\n",
      "       'DDoS-ICMP_Fragmentation', 'Recon-PortScan', 'DNS_Spoofing',\n",
      "       'DDoS-UDP_Fragmentation', 'Recon-OSScan', 'XSS', 'DDoS-HTTP_Flood',\n",
      "       'Recon-HostDiscovery', 'CommandInjection', 'VulnerabilityScan',\n",
      "       'DDoS-SlowLoris', 'Backdoor_Malware', 'BrowserHijacking',\n",
      "       'DictionaryBruteForce', 'SqlInjection', 'Recon-PingSweep',\n",
      "       'Uploading_Attack'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_target)\n",
    "print(train_target_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor = torch.FloatTensor(train_features.values)\n",
    "train_y_tensor = torch.from_numpy(train_target)\n",
    "\n",
    "test_X_tensor = torch.FloatTensor(test_features.values)\n",
    "test_y_tensor = torch.from_numpy(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X is features, y is targets\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        a = self.X[idx]\n",
    "        b = self.y[idx]\n",
    "        return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_X_tensor, train_y_tensor)\n",
    "test_dataset = CustomDataset(test_X_tensor, test_y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size) #shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# setting up the device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "# Using mps device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=46, out_features=46, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=46, out_features=34, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(46,46),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(46,34)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device=device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss fucntion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the model parameters\n",
    "# To train a model, we need a loss function and an optimizer.\n",
    "lr = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define train and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# In a single training loop, the model makes predictinos on the training dataset (fed to it in batches)\n",
    "# and backpropagates the predictino error to adjust the model's parameters\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute predictino error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 20000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}|{size:>5d}]\")\n",
    "\n",
    "# We also check the model's performance against the test dataset to ensure it is learning.\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3842386.250000 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.376587\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 3.220924 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.309892\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 3.054395 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.283809\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 2.963405 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.271794\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------\n",
      "loss: 2.908771 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.264430\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------\n",
      "loss: 2.872543 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.258856\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------\n",
      "loss: 2.846604 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.253622\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------\n",
      "loss: 2.826982 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.248756\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------\n",
      "loss: 2.811547 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.244026\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------\n",
      "loss: 2.799066 [   32|238687]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.240985\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# The training process is conducted over several iterations (epochs).\n",
    "# Durin each epoch, the model learns parameters to make better predictions.\n",
    "# Print the model's accuracy and loss at each epoch.\n",
    "# see the accuracy increase and the loss cecrease with every epoch.\n",
    "# the train and test takes approximately [6] mins in M1 mac (mps).\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')\n",
    "\n",
    "# load model\n",
    "# loaded_model = torch.load('model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
