{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = './part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv'\n",
    "raw_data = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc: integer location [row, column]\n",
    "# store only data (not column name)\n",
    "# use iloc, not just slicing.\n",
    "data = raw_data.iloc[1:, :-1]\n",
    "label = raw_data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.shape)\n",
    "# print(data.head(5))\n",
    "# print(label.shape)\n",
    "# print(label.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label = label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(unique_label))\n",
    "# print(unique_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate',\n",
      "       'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
      "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
      "       'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count',\n",
      "       'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet',\n",
      "       'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC',\n",
      "       'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number',\n",
      "       'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "DDoS-ICMP_Flood            36554\n",
      "DDoS-UDP_Flood             27626\n",
      "DDoS-TCP_Flood             23149\n",
      "DDoS-PSHACK_Flood          21210\n",
      "DDoS-SYN_Flood             20739\n",
      "DDoS-RSTFINFlood           20669\n",
      "DDoS-SynonymousIP_Flood    18189\n",
      "DoS-UDP_Flood              16957\n",
      "DoS-TCP_Flood              13630\n",
      "DoS-SYN_Flood              10275\n",
      "BenignTraffic               5600\n",
      "Mirai-greeth_flood          5016\n",
      "Mirai-udpplain              4661\n",
      "Mirai-greip_flood           3758\n",
      "DDoS-ICMP_Fragmentation     2377\n",
      "MITM-ArpSpoofing            1614\n",
      "DDoS-ACK_Fragmentation      1505\n",
      "DDoS-UDP_Fragmentation      1484\n",
      "DNS_Spoofing                 925\n",
      "Recon-HostDiscovery          697\n",
      "Recon-OSScan                 517\n",
      "Recon-PortScan               430\n",
      "DoS-HTTP_Flood               414\n",
      "VulnerabilityScan            210\n",
      "DDoS-HTTP_Flood              169\n",
      "DDoS-SlowLoris               106\n",
      "DictionaryBruteForce          63\n",
      "SqlInjection                  31\n",
      "BrowserHijacking              30\n",
      "CommandInjection              28\n",
      "Backdoor_Malware              22\n",
      "XSS                           18\n",
      "Uploading_Attack               8\n",
      "Recon-PingSweep                6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = raw_data['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = raw_data.drop('label', axis = 1)\n",
    "target = raw_data['label']\n",
    "\n",
    "# print(len(features))\n",
    "# print(features)\n",
    "# print(len(target))\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For label, do label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, target_mapping = pd.factorize(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 2]\n",
      "Index(['DDoS-RSTFINFlood', 'DoS-TCP_Flood', 'DDoS-ICMP_Flood', 'DoS-UDP_Flood',\n",
      "       'DoS-SYN_Flood', 'Mirai-greeth_flood', 'DDoS-SynonymousIP_Flood',\n",
      "       'Mirai-udpplain', 'DDoS-SYN_Flood', 'DDoS-PSHACK_Flood',\n",
      "       'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'BenignTraffic', 'MITM-ArpSpoofing',\n",
      "       'DDoS-ACK_Fragmentation', 'Mirai-greip_flood', 'DoS-HTTP_Flood',\n",
      "       'DDoS-ICMP_Fragmentation', 'Recon-PortScan', 'DNS_Spoofing',\n",
      "       'DDoS-UDP_Fragmentation', 'Recon-OSScan', 'XSS', 'DDoS-HTTP_Flood',\n",
      "       'Recon-HostDiscovery', 'CommandInjection', 'VulnerabilityScan',\n",
      "       'DDoS-SlowLoris', 'Backdoor_Malware', 'BrowserHijacking',\n",
      "       'DictionaryBruteForce', 'SqlInjection', 'Recon-PingSweep',\n",
      "       'Uploading_Attack'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(target)\n",
    "print(target_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide dataset into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(features_train))\n",
    "\n",
    "# Since it's still pandas dataframe, translate into numpy array and then Tensor.\n",
    "print(type(features_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = torch.Tensor(features_train.values)\n",
    "features_test = torch.Tensor(features_test.values)\n",
    "\n",
    "target_train = torch.Tensor(target_train)\n",
    "target_test = torch.Tensor(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(features_train, target_train)\n",
    "test_dataset = TensorDataset(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size) #shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_tensor = torch.from_numpy(features.values)\n",
    "# # X_tensor = torch.FloatTensor(features.values)\n",
    "# # y_tensor = torch.from_numpy(target)\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         # X is features, y is targets\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         a = self.X[idx]\n",
    "#         b = self.y[idx]\n",
    "#         return a, b\n",
    "\n",
    "# dataset = CustomDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# setting up the device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "# Using mps device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=46, out_features=46, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=46, out_features=34, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(46,46),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(46,34)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device=device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define loss fucntion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the model parameters\n",
    "# To train a model, we need a loss function and an optimizer.\n",
    "lr = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define train and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# In a single training loop, the model makes predictinos on the training dataset (fed to it in batches)\n",
    "# and backpropagates the predictino error to adjust the model's parameters\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute predictino error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}|{size:>5d}]\")\n",
    "\n",
    "# We also check the model's performance against the test dataset to ensure it is learning.\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------\n",
      "loss: 3873877.500000 [   64|238687]\n",
      "loss: 3.558402 [ 6464|238687]\n",
      "loss: 3.532887 [12864|238687]\n",
      "loss: 3.546443 [19264|238687]\n",
      "loss: 3.528836 [25664|238687]\n",
      "loss: 3.508924 [32064|238687]\n",
      "loss: 3.524583 [38464|238687]\n",
      "loss: 3.515392 [44864|238687]\n",
      "loss: 3.506018 [51264|238687]\n",
      "loss: 3.500764 [57664|238687]\n",
      "loss: 3.499952 [64064|238687]\n",
      "loss: 3.481689 [70464|238687]\n",
      "loss: 3.481556 [76864|238687]\n",
      "loss: 3.483852 [83264|238687]\n",
      "loss: 3.457346 [89664|238687]\n",
      "loss: 3.480426 [96064|238687]\n",
      "loss: 3.444626 [102464|238687]\n",
      "loss: 3.460358 [108864|238687]\n",
      "loss: 3.453791 [115264|238687]\n",
      "loss: 3.419555 [121664|238687]\n",
      "loss: 3.442797 [128064|238687]\n",
      "loss: 3.437433 [134464|238687]\n",
      "loss: 3.416757 [140864|238687]\n",
      "loss: 3.429369 [147264|238687]\n",
      "loss: 3.421117 [153664|238687]\n",
      "loss: 3.412008 [160064|238687]\n",
      "loss: 3.412659 [166464|238687]\n",
      "loss: 3.380772 [172864|238687]\n",
      "loss: 3.413282 [179264|238687]\n",
      "loss: 3.364743 [185664|238687]\n",
      "loss: 3.396656 [192064|238687]\n",
      "loss: 3.368192 [198464|238687]\n",
      "loss: 3.375865 [204864|238687]\n",
      "loss: 3.394939 [211264|238687]\n",
      "loss: 3.364663 [217664|238687]\n",
      "loss: 3.375353 [224064|238687]\n",
      "loss: 3.353481 [230464|238687]\n",
      "loss: 3.350866 [236864|238687]\n",
      "Epoch 2\n",
      "--------------------------------\n",
      "loss: 3.348607 [   64|238687]\n",
      "loss: 3.346058 [ 6464|238687]\n",
      "loss: 3.322095 [12864|238687]\n",
      "loss: 3.315712 [19264|238687]\n",
      "loss: 3.327140 [25664|238687]\n",
      "loss: 3.321748 [32064|238687]\n",
      "loss: 3.355729 [38464|238687]\n",
      "loss: 3.326511 [44864|238687]\n",
      "loss: 3.324097 [51264|238687]\n",
      "loss: 3.320692 [57664|238687]\n",
      "loss: 3.347468 [64064|238687]\n",
      "loss: 3.287143 [70464|238687]\n",
      "loss: 3.285675 [76864|238687]\n",
      "loss: 3.289203 [83264|238687]\n",
      "loss: 3.262357 [89664|238687]\n",
      "loss: 3.314104 [96064|238687]\n",
      "loss: 3.256903 [102464|238687]\n",
      "loss: 3.299853 [108864|238687]\n",
      "loss: 3.299946 [115264|238687]\n",
      "loss: 3.223505 [121664|238687]\n",
      "loss: 3.266194 [128064|238687]\n",
      "loss: 3.253165 [134464|238687]\n",
      "loss: 3.238925 [140864|238687]\n",
      "loss: 3.286922 [147264|238687]\n",
      "loss: 3.270115 [153664|238687]\n",
      "loss: 3.251978 [160064|238687]\n",
      "loss: 3.240959 [166464|238687]\n",
      "loss: 3.191402 [172864|238687]\n",
      "loss: 3.268333 [179264|238687]\n",
      "loss: 3.173892 [185664|238687]\n",
      "loss: 3.233099 [192064|238687]\n",
      "loss: 3.192527 [198464|238687]\n",
      "loss: 3.208433 [204864|238687]\n",
      "loss: 3.240053 [211264|238687]\n",
      "loss: 3.203242 [217664|238687]\n",
      "loss: 3.230185 [224064|238687]\n",
      "loss: 3.197269 [230464|238687]\n",
      "loss: 3.190765 [236864|238687]\n",
      "Epoch 3\n",
      "--------------------------------\n",
      "loss: 3.189124 [   64|238687]\n",
      "loss: 3.180649 [ 6464|238687]\n",
      "loss: 3.159150 [12864|238687]\n",
      "loss: 3.134761 [19264|238687]\n",
      "loss: 3.171568 [25664|238687]\n",
      "loss: 3.178659 [32064|238687]\n",
      "loss: 3.227621 [38464|238687]\n",
      "loss: 3.181697 [44864|238687]\n",
      "loss: 3.184880 [51264|238687]\n",
      "loss: 3.183281 [57664|238687]\n",
      "loss: 3.233092 [64064|238687]\n",
      "loss: 3.137088 [70464|238687]\n",
      "loss: 3.134771 [76864|238687]\n",
      "loss: 3.140457 [83264|238687]\n",
      "loss: 3.113051 [89664|238687]\n",
      "loss: 3.188355 [96064|238687]\n",
      "loss: 3.113074 [102464|238687]\n",
      "loss: 3.178405 [108864|238687]\n",
      "loss: 3.181929 [115264|238687]\n",
      "loss: 3.073722 [121664|238687]\n",
      "loss: 3.131796 [128064|238687]\n",
      "loss: 3.112169 [134464|238687]\n",
      "loss: 3.103570 [140864|238687]\n",
      "loss: 3.180778 [147264|238687]\n",
      "loss: 3.158375 [153664|238687]\n",
      "loss: 3.132799 [160064|238687]\n",
      "loss: 3.110010 [166464|238687]\n",
      "loss: 3.047125 [172864|238687]\n",
      "loss: 3.159977 [179264|238687]\n",
      "loss: 3.028649 [185664|238687]\n",
      "loss: 3.109390 [192064|238687]\n",
      "loss: 3.058377 [198464|238687]\n",
      "loss: 3.080988 [204864|238687]\n",
      "loss: 3.123897 [211264|238687]\n",
      "loss: 3.081129 [217664|238687]\n",
      "loss: 3.122261 [224064|238687]\n",
      "loss: 3.080942 [230464|238687]\n",
      "loss: 3.068913 [236864|238687]\n",
      "Epoch 4\n",
      "--------------------------------\n",
      "loss: 3.069702 [   64|238687]\n",
      "loss: 3.055790 [ 6464|238687]\n",
      "loss: 3.038013 [12864|238687]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# test(test_dataloader, model, loss_fn)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[72], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The training process is conducted over several iterations (epochs).\n",
    "# Durin each epoch, the model learns parameters to make better predictions.\n",
    "# Print the model's accuracy and loss at each epoch.\n",
    "# see the accuracy increase and the loss cecrease with every epoch.\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
